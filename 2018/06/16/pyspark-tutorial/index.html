<!DOCTYPE html>






  


<html class="theme-next mist use-motion" lang="">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2"/>
<meta name="theme-color" content="#222">












<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />






















<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=6.2.0" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=6.2.0">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=6.2.0">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=6.2.0">


  <link rel="mask-icon" href="/images/logo.svg?v=6.2.0" color="#222">









<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    version: '6.2.0',
    sidebar: {"position":"left","display":"hide","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: false,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>


  




  <meta name="description" content="最近完成了Spark平台的欺诈模型开发，语言为PySpark，断断续续历时三个月，中间踩了很多坑，由于公司也没有特别懂应用这块的，最后和平台基础架构的同事们一起摸索完成了这个项目。Spark版本1.6，对dataframe的支持仍不够完善，在2.0+版本里优化了不少，这也是这个项目较为苦逼的地方。Python版本2.7。Spark设计之初应该是给程序员用的，和R/matlab这种给统计学/数学生用">
<meta property="og:type" content="article">
<meta property="og:title" content="PySpark防坑指南">
<meta property="og:url" content="http://yoursite.com/2018/06/16/pyspark-tutorial/index.html">
<meta property="og:site_name" content="夏小雨">
<meta property="og:description" content="最近完成了Spark平台的欺诈模型开发，语言为PySpark，断断续续历时三个月，中间踩了很多坑，由于公司也没有特别懂应用这块的，最后和平台基础架构的同事们一起摸索完成了这个项目。Spark版本1.6，对dataframe的支持仍不够完善，在2.0+版本里优化了不少，这也是这个项目较为苦逼的地方。Python版本2.7。Spark设计之初应该是给程序员用的，和R/matlab这种给统计学/数学生用">
<meta property="og:locale" content="default">
<meta property="og:updated_time" content="2018-06-17T02:52:41.558Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="PySpark防坑指南">
<meta name="twitter:description" content="最近完成了Spark平台的欺诈模型开发，语言为PySpark，断断续续历时三个月，中间踩了很多坑，由于公司也没有特别懂应用这块的，最后和平台基础架构的同事们一起摸索完成了这个项目。Spark版本1.6，对dataframe的支持仍不够完善，在2.0+版本里优化了不少，这也是这个项目较为苦逼的地方。Python版本2.7。Spark设计之初应该是给程序员用的，和R/matlab这种给统计学/数学生用">






  <link rel="canonical" href="http://yoursite.com/2018/06/16/pyspark-tutorial/"/>



<script type="text/javascript" id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  <title>PySpark防坑指南 | 夏小雨</title>
  









  <noscript>
  <style type="text/css">
    .use-motion .motion-element,
    .use-motion .brand,
    .use-motion .menu-item,
    .sidebar-inner,
    .use-motion .post-block,
    .use-motion .pagination,
    .use-motion .comments,
    .use-motion .post-header,
    .use-motion .post-body,
    .use-motion .collection-title { opacity: initial; }

    .use-motion .logo,
    .use-motion .site-title,
    .use-motion .site-subtitle {
      opacity: initial;
      top: initial;
    }

    .use-motion {
      .logo-line-before i { left: initial; }
      .logo-line-after i { right: initial; }
    }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="default">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">夏小雨</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button aria-label="Toggle navigation bar">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>




<nav class="site-nav">
  
    <ul id="menu" class="menu">
      
        
        
        
          
          <li class="menu-item menu-item-home">
    <a href="/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-home"></i> <br />Home</a>
  </li>
        
        
        
          
          <li class="menu-item menu-item-about">
    <a href="/about/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-user"></i> <br />About</a>
  </li>
        
        
        
          
          <li class="menu-item menu-item-archives">
    <a href="/archives/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />Archives</a>
  </li>

      
      
    </ul>
  

  
    

  

  
</nav>



  



</div>
    </header>

    


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/06/16/pyspark-tutorial/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="夏小雨">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="夏小雨">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">PySpark防坑指南
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2018-06-16 20:11:55" itemprop="dateCreated datePublished" datetime="2018-06-16T20:11:55+08:00">2018-06-16</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Edited on</span>
                
                <time title="Modified: 2018-06-17 10:52:41" itemprop="dateModified" datetime="2018-06-17T10:52:41+08:00">2018-06-17</time>
              
            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>最近完成了Spark平台的欺诈模型开发，语言为PySpark，断断续续历时三个月，中间踩了很多坑，由于公司也没有特别懂应用这块的，最后和平台基础架构的同事们一起摸索完成了这个项目。Spark版本1.6，对dataframe的支持仍不够完善，在2.0+版本里优化了不少，这也是这个项目较为苦逼的地方。Python版本2.7。Spark设计之初应该是给程序员用的，和R/matlab这种给统计学/数学生用的工具思维不太一样，所以习惯了用R用数据挖掘和建模项目的同学，可能要适应一下。以下内容并不保证完全正确，因为我对spark的接触也不是很深，所以有些理解可能是偏颇的，欢迎指正。</p>
<a id="more"></a>
<h2 id="读数据"><a href="#读数据" class="headerlink" title="读数据"></a>读数据</h2><p>基本上我都是从hive的表里读数据的。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建一个hive context</span></span><br><span class="line">hive_context = HiveContext(sc)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 写sql读hive table，生成的df是spark dataframe，不是rdd</span></span><br><span class="line">df = hive_context.sql(<span class="string">"select * from schema.tablename distribute by rand()"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 对spark dataframe使用sql，注册temptable，和hive中的表一起使用</span></span><br><span class="line">df.registerTempTable(<span class="string">"df_table"</span>)</span><br><span class="line">df2 = hive_context.sql(<span class="string">"select * from df_table a join schema.hivetablename b \</span></span><br><span class="line"><span class="string">on a.id = b.id distribute by rand()"</span>)</span><br></pre></td></tr></table></figure>
<p><strong>坑1</strong>：即便hive里的表没有数据倾斜的问题，如果不加<code>distribute by rand()</code>这句，有可能会造成spark dataframe数据倾斜，因此建议每次从hive通过sql读数据都要加上这句。<br><strong>坑2</strong>：不要在一个pyspark会话中创建两个hive context，即便是一样的名字都不行，这样可能会造成通过registerTempTable的table报”table not found”。<br><strong>坑3</strong>：换行需要加<code>\</code><br><strong>建议</strong>：当所需的数据来自多个表时，建议在hive中建好临时表，然后在pyspark中直接读临时表的数据，这比在pyspark中写复杂sql生成dataframe的速度要快。但我没有研究过为什么会这样，只是对我们的任务来说是如此。</p>
<h2 id="缓存"><a href="#缓存" class="headerlink" title="缓存"></a>缓存</h2><p>如果一个dataframe/rdd会被多次用到，可以将它缓存到内存里，这会提升计算速度。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cache</span></span><br><span class="line">df1.cache()</span><br><span class="line"></span><br><span class="line"><span class="comment"># delete from cache</span></span><br><span class="line">df1.unpersist()</span><br><span class="line"></span><br><span class="line"><span class="comment">#检查是否已经cache了</span></span><br><span class="line">df1.is_cached</span><br></pre></td></tr></table></figure></p>
<p><strong>坑</strong>：cache完了之后，以后再对这个df1做操作，不能将新生成的dataframe命名为df1，需命名为df2，否则会丢失对df1的缓存（其实df1仍然缓存着，只是再也找不到它了）。</p>
<p><strong>经验</strong>：cache的dataframe可以到application UI的storage里看到。</p>
<h2 id="数据清洗与变量加工"><a href="#数据清洗与变量加工" class="headerlink" title="数据清洗与变量加工"></a>数据清洗与变量加工</h2><p>在做数据清洗和变量加工的时候，spark没有R/matlab的Vectorization（向量化）的思维，可能因为R/matlab的基本元素是vector的关系吧。因此，有些R中一条命令就可以实现的效果，在spark中需要曲线救国。如将两个行数一样的dataframe合并到一起，R中只需要用<code>rbind</code>，但pypark中只能用join，需要先在两个dataframe里各创建一个id字段作为行的序号，再join on这个id字段。pyspark 1.6中虽然有<code>monotonically_increasing_id</code>这个函数，但它生成的序列只保证monotonically increasing，不能保证两张dataframe的序号是一样的，因此无法用它当作id。这是个令人有点无语的函数，但幸好2.0+的版本已经改进了。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 新增一列</span></span><br><span class="line">df = df.withColumn(<span class="string">'new_column'</span>, substring(<span class="string">'old_column'</span>, <span class="number">1</span>, <span class="number">32</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 也可以直接修改原column的数据，保持列名不变</span></span><br><span class="line">df = df.withColumn(<span class="string">'old_column'</span>, substring(<span class="string">'old_column'</span>, <span class="number">1</span>, <span class="number">32</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 把两列的值拼起来，生成一个新的列</span></span><br><span class="line">df = df.withColumn(<span class="string">'ip_pt'</span>,concat(df.ip, df.pt))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 根据df中其他列的值是否符合条件，新增一列</span></span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> functions <span class="keyword">as</span> F</span><br><span class="line">df = df.withColumn(<span class="string">'flag_diff'</span>, F.when(df.col1 == df.col2, <span class="number">0</span>).otherwise(<span class="number">1</span>)) <span class="comment"># 判断col1与col2是否不一致，一致为0，不一致为1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 根据df中A列的值是否在另一个dataframe的B列中，新增一列</span></span><br><span class="line">df = df.withColumn(<span class="string">'flag1'</span>, F.when(df.userid.isin(df2.select(<span class="string">'userid'</span>)\</span><br><span class="line">.collect()[<span class="number">0</span>].userid ), <span class="number">1</span>).otherwise(df.flag1)) </span><br><span class="line"></span><br><span class="line"><span class="comment"># filter column</span></span><br><span class="line">df = df.filter(df.col1 != <span class="string">'xxx'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># left join 两个dataframe</span></span><br><span class="line">df = df1.join(df2, <span class="string">'common_column'</span>, <span class="string">'left_outer'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># left join以后drop column from one dataframe</span></span><br><span class="line">df = df1.join(df2.select(col(<span class="string">'mob7'</span>), col(<span class="string">'mob_province'</span>).alias(<span class="string">'reg_mob_pro'</span>), col(<span class="string">'mob_city'</span>).alias(<span class="string">'reg_mob_city'</span>)), substring(df1.reg_mobile, <span class="number">1</span>, <span class="number">7</span>) == df2.mob7, <span class="string">'left_outer'</span>).drop(df2.mob7)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 和sql中count groupby having同等，groupby后保留count大于等于2的记录</span></span><br><span class="line">df12 = df1.groupby(<span class="string">'group'</span>).count().filter(<span class="string">"`count`&gt;= 2"</span>) </span><br><span class="line"></span><br><span class="line"><span class="comment"># groupby后重命名column</span></span><br><span class="line">df2 = df1.groupby(<span class="string">'group'</span>).agg(sum(<span class="string">'count'</span>).alias(<span class="string">'new_column_name'</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># groupby后长表变宽表</span></span><br><span class="line">df2 = df1.groupby(<span class="string">'col1'</span>, <span class="string">'col2'</span>).pivot(<span class="string">'col1'</span>).agg(avg(<span class="string">'col2'</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将na替换成别的值</span></span><br><span class="line">df = df.na.fill(<span class="number">0</span>, subset = (<span class="string">'selected_column'</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># drop column</span></span><br><span class="line">df = df.drop(<span class="string">'column_to_drop'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 改变column类型，如变成整型</span></span><br><span class="line"><span class="keyword">from</span> pyspark.sql.types <span class="keyword">import</span> IntegerType</span><br><span class="line">df1.select(<span class="string">'col_name'</span>).cast(IntegerType())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 合并两个行数一样的dataframe</span></span><br><span class="line">df1 = df1.withColumn(<span class="string">"id"</span>, lit(<span class="number">1</span>))</span><br><span class="line">df1 = df1.withColumn(<span class="string">"new_id"</span>, row_number().over(Window.partitionBy(<span class="string">"id"</span>)))</span><br><span class="line">df2 = df2.withColumn(<span class="string">"id"</span>, lit(<span class="number">1</span>))</span><br><span class="line">df2 = df2.withColumn(<span class="string">"new_id"</span>, row_number().over(Window.partitionBy(<span class="string">"id"</span>)))</span><br><span class="line">df = df.drop(<span class="string">'id'</span>).join(df2.drop(<span class="string">'id'</span>), <span class="string">"new_id"</span>, <span class="string">"outer"</span>).drop(<span class="string">'new_id'</span>)</span><br></pre></td></tr></table></figure>
<h3 id="N个dataframe做left-join"><a href="#N个dataframe做left-join" class="headerlink" title="N个dataframe做left join"></a>N个dataframe做left join</h3><p>单独把这个问题拿出来，是因为这个操作导致的内存溢出问题研究了很久都不能解决，后来在开发的帮助下换成rdd操作才不报错了。</p>
<p>为什么会有N个dataframe做left join的需求呢？在训练模型的时候，我们需要一个training set，包含所有模型中要用到的变量。在我们的团伙欺诈模型中，每个变量是group维度的变量，而我们的原始数据来自交易表，交易是单个用户维度的变量，所以我们需要将用户维度的变量加工成group维度的变量，如计算group中某个属性的比例，其中涉及到并不复杂但大量的groupby和计算百分比的工作。我以往在R中的处理方式是对每个变量单独做加工，存在一个dataframe中，每个dataframe的行数是一样的，都是group的个数，然后将这些dataframe merge到一起，得到training set。然而，这个方法在pyspark中会报out of memory。每个dataframe是从一个500w行的交易表做groupby加工来的，得到一个(2w,2)的dataframe，15个这样的dataframe做left join就会报out of memory，究其原因，是因为spark是lazy computation，在不进行action操作的时候，它只记录了要做的transform。15个groupby，pivot，left join都是基于同一个原始dataframe进行的transform，导致整个chain特别长，并且循环使用同一个dataframe，结果是仅仅展示DAG就让浏览器崩溃了。</p>
<p>对于这个问题，spark给出的方案是在中间某个时刻做checkpoint。然而，在1.6版本中只有rdd可以做checkpoint，dataframe不行。当我们把dataframe转换成rdd，将所有的计算和操作在rdd上完成，最后将rdd再转换成dataframe，就不再报错了，并且速度可以接受。一个合理的解释是spark 1.6对dataframe的优化不够，所以在操作dataframe报无法解决的out of memory的问题时，改成rdd可能是一个行之有效的解决方法。</p>

      
    </div>

    

    
    
    

    

    

    

    <footer class="post-footer">
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/06/07/graph-in-fraud-detection/" rel="next" title="团伙交易欺诈模型实践">
                <i class="fa fa-chevron-left"></i> 团伙交易欺诈模型实践
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">夏小雨</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/archives/">
                
                    <span class="site-state-item-count">7</span>
                    <span class="site-state-item-name">posts</span>
                  </a>
                </div>
              

              

              
            </nav>
          

          

          

          
          

          
          

          
            
          
          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#读数据"><span class="nav-number">1.</span> <span class="nav-text">读数据</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#缓存"><span class="nav-number">2.</span> <span class="nav-text">缓存</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#数据清洗与变量加工"><span class="nav-number">3.</span> <span class="nav-text">数据清洗与变量加工</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#N个dataframe做left-join"><span class="nav-number">3.1.</span> <span class="nav-text">N个dataframe做left join</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2018</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">夏小雨</span>

  

  
</div>




  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> v3.7.1</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/theme-next/hexo-theme-next">NexT.Mist</a> v6.2.0</div>




        








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>


























  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=6.2.0"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=6.2.0"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=6.2.0"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=6.2.0"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=6.2.0"></script>



  



	





  





  










  





  

  

  

  

  
  

  

  

  

  

  

</body>
</html>
